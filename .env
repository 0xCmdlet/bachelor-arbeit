# MinIO
MINIO_ENDPOINT=minio:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin123
MINIO_BUCKET=study
MINIO_SECURE=false

# Postgres
POSTGRES_DSN=postgresql://admin:admin123@postgres:5432/mydb

# Qdrant
QDRANT_URL=http://qdrant:6333
QDRANT_COLLECTION=docs_docling_langchain_nomic_hybrid_rerank
# QDRANT_API_KEY=

# Extraction Strategy
# Options: "docling" (default), "unstructured", or "llamaparse"
EXTRACTOR_TYPE=docling

# Unstructured.io Configuration (only used if EXTRACTOR_TYPE=unstructured)
UNSTRUCTURED_API_KEY=your_unstructured_api_key_here
UNSTRUCTURED_PARTITION_STRATEGY=auto
UNSTRUCTURED_CHUNKING_STRATEGY=by_title
UNSTRUCTURED_MAX_CHARACTERS=1200
UNSTRUCTURED_OVERLAP=150

# LlamaParse Configuration (only used if EXTRACTOR_TYPE=llamaparse)
LLAMA_CLOUD_API_KEY=your_llama_cloud_api_key_here
LLAMAPARSE_PARSE_MODE=parse_page_with_agent
LLAMAPARSE_MODEL=openai-gpt-4-1-mini

# Embeddings / Chunking
# Embedding Provider: "sentence-transformers" or "openai"
EMBEDDING_PROVIDER=sentence-transformers

# SentenceTransformer Models (only used if EMBEDDING_PROVIDER=sentence-transformers)
EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5
# EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# OpenAI Embeddings (only used if EMBEDDING_PROVIDER=openai)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# Chunking Strategy (applies to Docling and LlamaParse only - Unstructured uses native chunking)
# Options: "langchain" (default), "chonkie", or "semantic"
CHUNKING_STRATEGY=langchain

# LangChain Chunking Configuration (only used if CHUNKING_STRATEGY=langchain)
CHUNK_TOKENS=750
CHUNK_OVERLAP=150

# Chonkie Chunking Configuration (only used if CHUNKING_STRATEGY=chonkie)
CHONKIE_CHUNK_SIZE=512
CHONKIE_OVERLAP=128

# Maximum chunk size (used for two-pass re-chunking in Chonkie and Semantic)
MAX_CHUNK_TOKENS=2000

# Retrieval Strategy
# Options: "dense" (default), "hybrid" (dense + BM25)
RETRIEVAL_STRATEGY=hybrid

# Hybrid Search Configuration (only used if RETRIEVAL_STRATEGY contains "hybrid")
HYBRID_ALPHA=0.5  # Weight: 0.0=pure sparse, 0.5=balanced, 1.0=pure dense

# Reranking Strategy (applies to both dense and hybrid)
# Options: "none" (default), "cross-encoder"
RERANKING_STRATEGY=cross-encoder
RERANKER_MODEL=cross-encoder/mmarco-mMiniLMv2-L12-H384-v1
RERANK_TOP_K_MULTIPLIER=3

# Worker
POLL_INTERVAL=5
DELETE_AFTER_INGEST=false

# HuggingFace Token (optional - only needed for gated models)
HUGGINGFACE_HUB_TOKEN=your_huggingface_token_here

# LLM / Ollama
LLM_MODEL=llama3.1:8b-instruct-q4_0
VLLM_URL=http://ollama:11434
OLLAMA_URL=http://ollama:11434
MAX_TOKENS=2048
TEMPERATURE=0.1

# ===== 24GB VRAM Profile (Optimized for Vast.ai RTX 3090) =====
# Use this profile for cloud GPU instances with 24GB VRAM
# Optimized for maximum performance and throughput

# Embedding batch size (8x increase for 24GB VRAM)
# Larger batches = faster embedding, less CPU↔GPU transfer overhead
EMBED_BATCH=128

# Max chunks to process per batch (6x increase for 24GB VRAM)
# Process more chunks before clearing GPU cache
MAX_CHUNKS_PER_BATCH=150

# Device for embedding models
# Force GPU for all embedding operations (plenty of VRAM available)
EMBEDDING_DEVICE=cuda

# Device for semantic chunking embeddings
# Force GPU for semantic chunking (3-5x faster than CPU)
# No risk of OOM with 24GB VRAM
SEMANTIC_CHUNKING_DEVICE=cuda

# GPU cache clearing interval
# Clear every 10 batches instead of every batch (less overhead)
GPU_CACHE_CLEAR_INTERVAL=10

# Clear GPU cache between pipeline phases
# Disabled to reduce overhead (24GB is enough for all phases)
GPU_CACHE_CLEAR_BETWEEN_PHASES=false

# Device for cross-encoder reranker
# Force GPU for reranking (20-30% faster than CPU)
RERANKER_DEVICE=cuda

# ===== Expected Performance Improvement =====
# With this profile on 24GB RTX 3090:
# - Semantic chunking: 3-5x faster (CPU → GPU)
# - Embedding generation: 1.5-2x faster (larger batches)
# - Reranking: 20-30% faster (CPU → GPU)
# - Overall evaluation: 6-10 hours (down from 12-24 hours)
# - Cost savings: ~$1.50-$2.50 on Vast.ai
