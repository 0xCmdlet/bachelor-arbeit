# RAG Pipeline Evaluation

Evaluation framework for comparing RAG pipeline variants using RAGAS metrics.

## Overview

This evaluation system analyzes stored pipeline test results to systematically compare different RAG configurations (extractors, chunking strategies, embeddings, retrieval methods) using standardized quality metrics.

**Key Features:**
- Offline evaluation (no need to re-run pipelines)
- RAGAS metrics: Context Precision, Faithfulness, Factual Correctness, Semantic Similarity
- Supports 46 German test queries about C programming
- Outputs: JSON (detailed), CSV (comparison), TXT (summary)

## Files

```
evaluation/
├── evaluate_pipeline_results.py    # Main evaluation script
├── ground_truth.json                # Reference answers for 46 test queries
├── requirements.txt                 # Python dependencies
├── .env                             # API keys (create this)
└── results/                         # Evaluation outputs
    ├── ragas_evaluation_*.json      # Detailed results
    ├── ragas_evaluation_*.csv       # Comparison table
    └── ragas_evaluation_*.txt       # Summary report
```

## Prerequisites

### 1. Install Dependencies

```bash
cd evaluation
pip install -r requirements.txt
```

**Required packages:**
- `ragas` - Evaluation framework
- `langchain-openai` - LLM integration
- `langchain-huggingface` - Embeddings
- `pandas` - Data processing
- `python-dotenv` - Environment variables

### 2. Set Up API Key

Create a `.env` file in the `evaluation/` folder:

```bash
echo "OPENAI_API_KEY=sk-your-api-key-here" > .env
```

**Why needed:** RAGAS uses GPT-4o-mini to evaluate metrics like Context Precision and Faithfulness.

### 3. Pipeline Test Results

Ensure you have pipeline test results in `../results/`:
- `pipeline_test_results.json` (full results, 6.1 MB)
- `small-sample.json` (test subset, 438 KB)

These are generated by `../pipeline-testing/test-pipelines.py`.

## Usage

### Basic Evaluation

Evaluate all pipelines with ground truth:

```bash
python evaluate_pipeline_results.py --ground-truth ground_truth.json
```

This will:
1. Load `../results/pipeline_test_results.json`
2. Evaluate all successful pipelines
3. Save results to `results/ragas_evaluation_YYYYMMDD_HHMMSS.*`

### Evaluate Subset (Quick Test)

Use the small sample for faster testing:

```bash
python evaluate_pipeline_results.py \
  --results-file ../results/small-sample.json \
  --ground-truth ground_truth.json
```

### Evaluate Top N Pipelines

Evaluate only the top 5 pipelines by retrieval score:

```bash
python evaluate_pipeline_results.py \
  --ground-truth ground_truth.json \
  --top-n 5
```

### Evaluate Specific Pipelines

Evaluate only specific pipeline IDs:

```bash
python evaluate_pipeline_results.py \
  --ground-truth ground_truth.json \
  --pipeline-ids "doc_lan_nomi_den,uns_sem_open_hyb,lla_cho_nomi_den_rnk"
```

### Custom Output Directory

```bash
python evaluate_pipeline_results.py \
  --ground-truth ground_truth.json \
  --output-dir custom_results/
```

## Command-Line Options

| Option | Default | Description |
|--------|---------|-------------|
| `--results-file` | `../results/pipeline_test_results.json` | Input pipeline test results |
| `--ground-truth` | None | Ground truth reference file (recommended) |
| `--pipeline-ids` | None | Comma-separated pipeline IDs to evaluate |
| `--top-n` | None | Evaluate only top N pipelines by score |
| `--output-dir` | `results` | Output directory for evaluation results |

## Evaluation Metrics

### 1. Context Precision
- **Measures:** Relevance of retrieved contexts to the query
- **Range:** 0.0 to 1.0 (higher is better)
- **Requires:** Ground truth reference
- **Interpretation:** How well the retrieval system finds relevant information

### 2. Faithfulness
- **Measures:** Whether the answer is grounded in retrieved contexts
- **Range:** 0.0 to 1.0 (higher is better)
- **Detects:** Hallucinations and unsupported claims
- **Interpretation:** How faithful the LLM is to the source material

### 3. Factual Correctness
- **Measures:** Accuracy of the answer compared to ground truth
- **Range:** 0.0 to 1.0 (higher is better)
- **Requires:** Ground truth reference
- **Interpretation:** Overall answer quality and correctness

### 4. Semantic Similarity
- **Measures:** Semantic similarity between generated and reference answers
- **Range:** 0.0 to 1.0 (higher is better)
- **Uses:** Embedding-based similarity
- **Interpretation:** How close the answer is to the expected answer

### Composite Score
Average of all four metrics, used for ranking pipelines.

## Output Files

After evaluation, three files are created in `results/`:

### 1. JSON File (`ragas_evaluation_YYYYMMDD_HHMMSS.json`)

Detailed results including:
- Metadata (timestamp, source file, metrics used)
- Per-pipeline scores (overall + per-query)
- Full query details (query text, answer, contexts, individual metric scores)

**Use for:** Deep analysis, debugging specific queries

### 2. CSV File (`ragas_evaluation_YYYYMMDD_HHMMSS_comparison.csv`)

Comparison table with columns:
- `pipeline_id`
- `extractor`, `chunking`, `embedding`, `retrieval`
- `avg_retrieval_score`, `avg_retrieval_time_ms`
- `ragas_faithfulness`, `ragas_context_precision`, `ragas_factual_correctness`, `ragas_semantic_similarity`
- `ragas_composite` (average of 4 metrics)

**Use for:** Excel/Pandas analysis, sorting, filtering, visualization

### 3. Summary Text (`ragas_evaluation_YYYYMMDD_HHMMSS_summary.txt`)

Human-readable summary:
- Metadata (timestamp, pipelines evaluated, metrics used)
- Top 10 pipelines ranked by composite score
- Individual metric scores for each pipeline
- Retrieval performance (score, time)

**Use for:** Quick overview, presentations, reports

## Cost Estimation

**Per Query:**
- 4 metrics × ~500 tokens/metric = ~2,000 tokens
- Using `gpt-4o-mini`: ~$0.001 per query

**Examples:**
- 3 pipelines × 46 queries = 138 evaluations → ~$0.14
- 10 pipelines × 46 queries = 460 evaluations → ~$0.46
- 72 pipelines × 46 queries = 3,312 evaluations → ~$3.30

## Example Workflow

### 1. Quick Test (3 pipelines)
```bash
# Use small sample for fast testing
python evaluate_pipeline_results.py \
  --results-file ../results/small-sample.json \
  --ground-truth ground_truth.json

# Cost: ~$0.14, Time: ~5 minutes
```

### 2. Top Performers (top 10)
```bash
# Evaluate best pipelines only
python evaluate_pipeline_results.py \
  --ground-truth ground_truth.json \
  --top-n 10

# Cost: ~$0.46, Time: ~15 minutes
```

### 3. Full Evaluation (all 72)
```bash
# Comprehensive evaluation
python evaluate_pipeline_results.py \
  --ground-truth ground_truth.json

# Cost: ~$3.30, Time: ~45 minutes
```

## Troubleshooting

### Error: "OPENAI_API_KEY not set"
**Solution:** Create `.env` file with your API key:
```bash
echo "OPENAI_API_KEY=sk-your-key" > .env
```

### Error: "Results file not found"
**Solution:** Generate pipeline results first:
```bash
cd ../pipeline-testing
python test-pipelines.py --limit 3
```

### Error: "No successful pipeline results to evaluate"
**Solution:** Check that `pipeline_test_results.json` contains:
- `"phase": "retrieval"`
- `"status": "success"`

### Evaluation is slow
**Tips:**
- Start with `--top-n 3` for testing
- Use `small-sample.json` instead of full results
- RAGAS metrics require API calls (unavoidable)
- Run during off-peak hours for better API response times

### Import errors (ragas, langchain, etc.)
**Solution:** Ensure you're using the evaluation virtual environment:
```bash
cd evaluation
source .venv/bin/activate  # Linux/Mac
pip install -r requirements.txt
```

## Ground Truth Format

The `ground_truth.json` file contains reference answers for 46 queries:

```json
[
  {
    "query_id": "q1",
    "reference": "C wurde 1974 von Dennis M. Ritchie entwickelt."
  },
  {
    "query_id": "q2",
    "reference": "1988: Normierung durch ANSI, 'ANSI-C'."
  }
]
```

**Fields:**
- `query_id`: Unique identifier (matches pipeline test results)
- `reference`: Expected/correct answer in German

## Pipeline IDs

Pipeline IDs follow the format: `{extractor}_{chunking}_{embedding}_{retrieval}[_reranking]`

**Examples:**
- `doc_lan_nomi_den` = Docling + LangChain + Nomic + Dense
- `uns_sem_open_hyb` = Unstructured + Semantic + OpenAI + Hybrid
- `lla_cho_nomi_den_rnk` = LlamaParse + Chonkie + Nomic + Dense + Reranking

**Components:**
- Extractor: `doc` (Docling), `uns` (Unstructured), `lla` (LlamaParse)
- Chunking: `lan` (LangChain), `cho` (Chonkie), `sem` (Semantic)
- Embedding: `nomi` (Nomic), `open` (OpenAI)
- Retrieval: `den` (Dense), `hyb` (Hybrid)
- Reranking: `rnk` (Cross-encoder reranking, optional)

## Next Steps

1. **Analyze Results:** Open the CSV file in Excel/Pandas for sorting and filtering
2. **Identify Best Pipeline:** Look for highest composite score
3. **Debug Poor Performers:** Check per-query JSON details for specific failures
4. **Iterate:** Adjust pipeline configurations based on insights
5. **Document Findings:** Use summary TXT for reports and presentations

## References

- [RAGAS Documentation](https://docs.ragas.io/)
- [RAGAS Metrics Guide](https://docs.ragas.io/en/latest/concepts/metrics/)
- [LangChain Integration](https://python.langchain.com/)
- Pipeline Testing: `../pipeline-testing/`
- Test Results: `../results/`
