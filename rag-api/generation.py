import os
import structlog
from typing import List, Dict, Any
import httpx
import json

logger = structlog.get_logger()

# Configuration
OLLAMA_URL = os.getenv("OLLAMA_URL", os.getenv("VLLM_URL", "http://localhost:11434"))
LLM_MODEL = os.getenv("LLM_MODEL", "llama3.1:8b-instruct-q4_0")
MAX_TOKENS = int(os.getenv("MAX_TOKENS", "2048"))
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.1"))

class LLMGenerator:
    def __init__(self):
        self.ollama_url = OLLAMA_URL
        self.max_tokens = MAX_TOKENS
        self.temperature = TEMPERATURE
        logger.info("llm_generator_initialized", url=self.ollama_url)

    async def generate_answer(self, query: str, context_docs: List[Dict[str, Any]]) -> str:
        """Generate answer using retrieved context documents"""
        try:
            # Build context from retrieved documents
            context = self._build_context(context_docs)

            # Create prompt with context and query
            prompt = self._create_prompt(query, context)

            # Generate response using Ollama
            response = await self._call_ollama(prompt)

            logger.info(
                "answer_generated",
                query=query,
                context_docs_count=len(context_docs),
                response_length=len(response)
            )

            return response

        except Exception as e:
            logger.error("answer_generation_failed", query=query, error=str(e))
            raise

    def _build_context(self, context_docs: List[Dict[str, Any]]) -> str:
        """Build context string from retrieved documents"""
        if not context_docs:
            return "No relevant documents found."

        context_parts = []
        for i, doc in enumerate(context_docs, 1):
            filename = doc.get('filename', 'Unknown')
            text = doc.get('text', '')
            score = doc.get('score', 0.0)

            # Truncate very long text to prevent prompt overflow
            if len(text) > 1000:
                text = text[:1000] + "..."

            context_parts.append(
                f"Document {i} (Source: {filename}, Relevance: {score:.3f}):\n{text}"
            )

        return "\n\n".join(context_parts)

    def _create_prompt(self, query: str, context: str) -> str:
        """Create prompt for the LLM with context and query"""
        system_prompt = """Du bist ein hilfreicher Assistent, der Fragen basierend auf den bereitgestellten Kontextdokumenten beantwortet.

Anweisungen:
- Verwende nur die Informationen aus den bereitgestellten Kontextdokumenten, um die Frage zu beantworten
- Wenn der Kontext nicht genügend Informationen enthält, um die Frage zu beantworten, sage dies klar und deutlich
- Sei präzise, aber umfassend in deiner Antwort
- Zitiere spezifische Quellen, wenn relevant
- Wenn mehrere Dokumente relevante Informationen enthalten, fasse die Informationen angemessen zusammen"""

        user_prompt = f"""Kontextdokumente:
{context}

Frage: {query}

Bitte gib eine umfassende Antwort basierend auf den oben genannten Kontextdokumenten."""

        # Format for Llama-3.1-8B-Instruct
        formatted_prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>

{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""

        return formatted_prompt

    async def _call_ollama(self, prompt: str) -> str:
        """Call Ollama API to generate response"""
        payload = {
            "model": LLM_MODEL,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "max_tokens": self.max_tokens,
            "temperature": self.temperature,
            "top_p": 0.9,
            "stop": ["<|eot_id|>"]
        }

        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{self.ollama_url}/v1/chat/completions",
                json=payload,
                headers={"Content-Type": "application/json"}
            )

            if response.status_code != 200:
                raise Exception(f"Ollama API error: {response.status_code} - {response.text}")

            result = response.json()

            if "choices" not in result or not result["choices"]:
                raise Exception("No response generated by Ollama")

            generated_text = result["choices"][0]["message"]["content"]

            # Clean up any remaining special tokens
            generated_text = generated_text.replace("<|eot_id|>", "").strip()

            return generated_text

    async def get_model_info(self) -> Dict[str, Any]:
        """Get information about the loaded model"""
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(f"{self.ollama_url}/v1/models")

                if response.status_code == 200:
                    return response.json()
                else:
                    return {"error": f"Failed to get model info: {response.status_code}"}

        except Exception as e:
            logger.error("model_info_failed", error=str(e))
            return {"error": str(e)}